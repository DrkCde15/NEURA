import ollama
import sqlite3
import pyfiglet

# --------------------------
# CONFIG
# --------------------------
DB_PATH = "data_memory.db"
MODEL = "gemma:2b"  # Altere para o modelo que voc√™ tem localmente

# --------------------------
# INICIALIZA√á√ÉO
# --------------------------
def display_banner():
    banner = pyfiglet.figlet_format("NEURA AI", font="small")
    print("=" * 60)
    print(banner)
    print("ü§ñ  Assistente IA com Mem√≥ria Local")

def display_help():
    """Exibe os comandos dispon√≠veis de forma organizada"""
    print("\n" + "üìã COMANDOS DISPON√çVEIS ".center(50, "="))
    print("üî∏ 'sair'    - Encerra o programa")
    print("üî∏ 'limpar'  - Limpa toda a mem√≥ria da conversa")
    print("üî∏ 'estado'  - Mostra estat√≠sticas da mem√≥ria")
    print("üî∏ 'ajuda'   - Mostra esta mensagem")
    print("=" * 50 + "\n")

# --------------------------
# FUN√á√ïES DE MEM√ìRIA
# --------------------------
def init_db():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS memory (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            role TEXT,
            content TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

def save_message(role, content):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("INSERT INTO memory (role, content) VALUES (?, ?)", (role, content))
    conn.commit()
    conn.close()

def load_memory(limit=3):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT role, content FROM memory ORDER BY id DESC LIMIT ?", (limit,))
    rows = cur.fetchall()
    conn.close()
    return rows[::-1]

def clear_memory():
    """Limpa toda a mem√≥ria do banco de dados"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("DELETE FROM memory")
    conn.commit()
    conn.close()
    print("üóëÔ∏è  Mem√≥ria limpa com sucesso!\n")

def get_memory_stats():
    """Retorna estat√≠sticas detalhadas da mem√≥ria"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    # Total de mensagens
    cur.execute("SELECT COUNT(*) FROM memory")
    total = cur.fetchone()[0]
    
    # Mensagens por role
    cur.execute("SELECT role, COUNT(*) FROM memory GROUP BY role")
    role_counts = dict(cur.fetchall())
    
    # √öltima mensagem
    cur.execute("SELECT timestamp FROM memory ORDER BY id DESC LIMIT 1")
    last_msg = cur.fetchone()
    
    conn.close()
    
    return {
        'total': total,
        'user_messages': role_counts.get('user', 0),
        'assistant_messages': role_counts.get('assistant', 0),
        'last_message': last_msg[0] if last_msg else 'Nenhuma'
    }

# --------------------------
# CHAMADA AO OLLAMA
# --------------------------
def call_ollama(prompt):
    try:
        response = ollama.generate(model=MODEL, prompt=prompt)
        return response['response']
    except Exception as e:
        return f"‚ùå Erro ao chamar o modelo: {str(e)}"

# --------------------------
# LOOP PRINCIPAL
# --------------------------
def main():
    # Exibe banner inicial
    display_banner()
    
    init_db()
    
    # Mostra estado inicial da mem√≥ria
    stats = get_memory_stats()
    print(f"üìä Mem√≥ria Inicial: {stats['total']} mensagens")
    print(f"üë§ Voc√™: {stats['user_messages']} | ü§ñ Neura: {stats['assistant_messages']}")
    print("‚îÄ" * 50)
    
    display_help()

    while True:
        user_msg = input("üí¨ Voc√™: ").strip()
        
        # Comando para sair
        if user_msg.lower() in ["sair", "exit", "quit", "quit()"]:
            print("\nüëã At√© mais! Encerrando Neura AI...")
            break
        
        # Comando para limpar mem√≥ria
        elif user_msg.lower() in ["limpar", "clear", "reset"]:
            clear_memory()
            continue
        
        # Comando para mostrar estado da mem√≥ria
        elif user_msg.lower() in ["estado", "status", "mem√≥ria", "memory", "stats"]:
            stats = get_memory_stats()
            print("\n" + "üìä ESTADO DA MEM√ìRIA ".center(50, "‚îÄ"))
            print(f"üìà Total de mensagens: {stats['total']}")
            print(f"üë§ Suas mensagens: {stats['user_messages']}")
            print(f"ü§ñ Respostas da Neura: {stats['assistant_messages']}")
            print(f"üïí √öltima mensagem: {stats['last_message']}")
            print("‚îÄ" * 50 + "\n")
            continue
        
        # Comando para ajuda
        elif user_msg.lower() in ["ajuda", "help", "comandos", "?"]:
            display_help()
            continue

        # Processa mensagem normal do usu√°rio
        save_message("user", user_msg)

        # Recupera mem√≥ria curta
        memory_blocks = load_memory(limit=3)
        
        SYSTEM_MESSAGE = (
            "Voc√™ √© a Neura, um assistente IA conversando em portugu√™s brasileiro. "
            "Seja claro, natural e prestativo. Responda sempre em portugu√™s."
        )

        # Monta prompt com contexto - CORRIGIDO
        full_prompt = f"CONTEXTO DO SISTEMA: {SYSTEM_MESSAGE}\n"
        full_prompt += "HIST√ìRICO RECENTE DA CONVERSA:\n"
        for role, content in memory_blocks:
            full_prompt += f"{role.upper()}: {content}\n"
        full_prompt += f"USER: {user_msg}\nASSISTANT: "

        # Chama modelo
        print("ü§ñ Neura: ", end="", flush=True)
        bot_response = call_ollama(full_prompt)
        print(bot_response + "\n")

        # Salva resposta
        save_message("assistant", bot_response)

if __name__ == "__main__":
    main()