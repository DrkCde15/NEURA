import os
import sqlite3
import logging
import requests
from typing import List, Optional, Dict
import ollama
from .image import NeuraVision
from .config import NeuraConfig

# Configura칞칚o de Logging
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("NeuraCore")

# Silencia logs verbosos de bibliotecas de terceiros
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

class Neura:
    def __init__(self, model: str = NeuraConfig.LLM_MODEL, 
                 vision_model: str = NeuraConfig.VISION_MODEL, 
                 system_prompt: str = "",
                 host: str = NeuraConfig.OLLAMA_BASE_URL,
                 headers: Optional[Dict[str, str]] = None):
        self.model = model
        self.vision_model = vision_model
        self.system_prompt = system_prompt
        self.db_path = NeuraConfig.DB_PATH
        self.host = host
        
        # Se estiver usando o t칰nel da Neura, adiciona os headers de bypass automaticamente
        self.headers = headers or {}
        if self.host == NeuraConfig.TUNNEL_URL:
            self.headers.update(NeuraConfig.BYPASS_HEADERS)
        
        # Inicializa o especialista em vis칚o com host e headers
        self.vision = NeuraVision(model=self.vision_model, host=self.host, headers=self.headers)
        
        # Inicializa o cliente Ollama com suporte a headers
        try:
            self.client = ollama.Client(host=self.host, headers=self.headers)
        except Exception as e:
            logger.error(f"Erro ao inicializar cliente Ollama: {e}")
            self.client = None
        
        # Inicializa o banco de dados
        self._init_db()

    def health_check(self) -> bool:
        """Verifica se o servidor de IA no host configurado est치 acess칤vel."""
        try:
            # Tenta conectar no host atual (pode ser local ou t칰nel)
            response = requests.get(f"{self.host}/api/tags", headers=self.headers, timeout=5)
            return response.status_code == 200
        except Exception:
            return False


    def _init_db(self) -> None:
        """Cria a tabela de mem칩ria se n칚o existir."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS memory (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        role TEXT,
                        content TEXT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                conn.commit()
        except sqlite3.Error as e:
            logger.critical(f"Erro ao inicializar banco de dados: {e}")

    def save_message(self, role: str, content: str) -> None:
        """Salva uma mensagem no hist칩rico do SQLite."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('INSERT INTO memory (role, content) VALUES (?, ?)', (role, content))
                conn.commit()
        except sqlite3.Error as e:
            logger.error(f"Erro ao salvar mensagem: {e}")

    def get_context(self, limit: int = 5) -> List[Dict[str, str]]:
        """Recupera as 칰ltimas mensagens para manter o contexto."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('SELECT role, content FROM memory ORDER BY id DESC LIMIT ?', (limit,))
                rows = cursor.fetchall()
                
                context = [{"role": "system", "content": self.system_prompt}]
                # Inverte para manter a ordem cronol칩gica
                for role, content in reversed(rows):
                    context.append({"role": role, "content": content})
                return context
        except sqlite3.Error as e:
            logger.error(f"Erro ao recuperar contexto: {e}")
            return [{"role": "system", "content": self.system_prompt}]

    def clear_memory(self) -> None:
        """Limpa o hist칩rico de conversas."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('DELETE FROM memory')
                conn.commit()
            logger.info("Mem칩ria resetada com sucesso.")
            print("Mem칩ria resetada!")
        except sqlite3.Error as e:
            logger.error(f"Erro ao limpar mem칩ria: {e}")

    def list_models(self) -> List[str]:
        """Lista os modelos dispon칤veis no Ollama local."""
        try:
            if not self.client:
                return []
            models_info = self.client.list()
            # Suporte para objetos Response do Ollama (vers칫es mais recentes)
            if hasattr(models_info, 'models'):
                return [m.model for m in models_info.models]
            # Fallback para dicion치rio (vers칫es antigas)
            return [m['name'] for m in models_info.get('models', [])]
        except Exception as e:
            logger.error(f"Erro ao listar modelos: {e}")
            return []

    def get_response(self, user_msg: str, image_path: Optional[str] = None) -> str:
        """Garante a resposta da IA, decidindo entre Texto ou Vis칚o."""
        try:
            # FLUXO 1: VIS츾O (Se houver imagem, delega ao NeuraVision)
            if image_path and os.path.exists(image_path):
                logger.info(f"Iniciando modo vis칚o para: {image_path}")
                print(f"Modo Vis칚o ativado...")
                # O m칩dulo image.py resolve o processamento pesado
                analise = self.vision.process_and_analyze(image_path, user_msg)
                
                # Salva a an치lise na mem칩ria para o contexto futuro
                if analise:
                    self.save_message("assistant", f"[游댌 Vis칚o]: {analise}")
                return analise

            # FLUXO 2: TEXTO
            self.save_message("user", user_msg)
            contexto = self.get_context()

            logger.info(f"Enviando prompt para LLM: {self.model}")
            response = self.client.chat(
                model=self.model,
                messages=contexto,
                options={"temperature": 0.3} # Baixa temperatura = Respostas mais realistas
            )

            final_text = response['message']['content'].strip()
            
            if final_text:
                self.save_message("assistant", final_text)
                return final_text
            
            logger.warning("LLM retornou resposta vazia.")
            return "Neura: N칚o consegui gerar uma resposta no momento."

        except Exception as e:
            logger.error(f"Erro cr칤tico no Core: {e}", exc_info=True)
            return f"Erro no Core: {str(e)}"